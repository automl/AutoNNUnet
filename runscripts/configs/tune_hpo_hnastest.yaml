# @package _global_
defaults:
  - _self_
  - base
  - cluster: kisski_gpu
  - search_space: hpo_hnas
  - dataset: Dataset001_BrainTumour
  - override hydra/sweeper: HyperNEPS

neps_seed: 0

architecture: (S unet (7E res_encoder (ENORM instance_norm) (ENONLIN leaky_relu) (EDROPOUT
  no_dropout) (1REB 1b) down (2REB 4b) down (3REB 8b) down (4REB 8b) down (5REB 6b)
  down (6REB 4b) down (7REB 4b)) (7D conv_decoder (DNORM instance_norm) (DNONLIN prelu)
  (DDROPOUT dropout) up (1DB 2b) up (2DB 2b) up (3DB 2b) up (4DB 3b) up (5DB 1b) up
  (6DB 2b)))

hp_config:
  num_epochs: 12.345679012345677
  total_epochs: 1000
  optimizer: AdamW
  momentum: 0.8411827156417522
  initial_lr: 0.0031615003563072743
  weight_decay: 2.9522869844756197e-05
  lr_scheduler: None
  oversample_foreground_percent: 0.00013327515718930538
  loss_function: DiceAndCrossEntropyLoss
  aug_factor: 1.2776741384729031
  aug_scale_prob: 0.2
  aug_rotate_prob: 0.2
  aug_gaussian_noise_prob: 0.1
  aug_gaussian_blur_prob: 0.2
  aug_brightness_prob: 0.15
  aug_contrast_prob: 0.15
  aug_lowres_prob: 0.25
  aug_gamma_1_prob: 0.1
  aug_gamma_2_prob: 0.3
  architecture: ${architecture}
  base_num_features: 60
  max_features: 370
  encoder_type: ConvolutionalEncoder
  activation: LeakyReLU
  dropout_rate: 0.26618207735809685
  model_scale: 1
  normalization: InstanceNorm

# HPO configuration
min_budget: 10    # Minimum number of epochs
max_budget: 1000  # Maximum number of epochs
eta: 3            # Reduction factor for successive halving
budget: 22_000    # Total budget in epochs

objectives: [loss, runtime]
maximize: [False, False]

# PriorBand
prior_confidence: medium

# HNAS config
s_max: 2

hydra:
  sweeper:
    budget: ${budget}
    budget_variable: hp_config.num_epochs
    search_space: ${search_space}
    loading_variable: load
    saving_variable: save
    sweeper_kwargs:
      env_vars: 
        NEPS_LOG_DIR: ${hydra.run.dir}
      objectives: ${objectives}
      maximize: ${maximize}
      checkpoint_tf: true
      load_tf: true
      checkpoint_path_typing: ""
      min_budget: ${min_budget}
      max_budget: ${max_budget}
      job_array_size_limit: 1   # only one config at a time
      seeds: [0, 1, 2, 3, 4]    # these are the folds in our case
      seed_keyword: fold
      # slurm: true
      # slurm_timeout: ${dataset.runtime_min_nas}
      deterministic: true
      optimizer_kwargs:
        fidelity_variable: ${hydra.sweeper.budget_variable}
        incumbent_selection: hypervolume
        architecture: autonnunet.hnas.get_architecture
        architecture_kwargs:
          n_stages: ${dataset.default_n_stages}
          s_max: ${s_max}
          prior_sampling_mode: distribution
          prior_confidence: ${prior_confidence}
        architecture_default: autonnunet.hnas.get_default_architecture
        architecture_default_kwargs:
          n_stages: ${dataset.default_n_stages}
        min_budget: ${min_budget}
        max_budget: ${max_budget}
        seed: ${neps_seed}
        optimizer:
          _target_: neps.optimizers.multi_objective.mo_priorband.MOPriorBand
          _partial_: true
          objectives: ${objectives}
          budget: ${hydra.sweeper.budget}
          eta: ${eta}
          prior_confidence: ${prior_confidence}
          prior_weight_type: geometric
          inc_sample_type: mutation
          inc_style: dynamic
          inc_mutation_rate: 0.5
          inc_mutation_std: 0.25
          model_based: False
          sample_default_first: True
          sample_default_at_target: True
  run:  
    dir: output/hpo_hnas_test/${dataset.name}/${trainer.configuration}/${neps_seed}
  sweep:
    dir: output/hpo_hnas_test/${dataset.name}/${trainer.configuration}/${neps_seed}
  job:
    chdir: true
